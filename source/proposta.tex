\chapter{Introdução} \label{chapter_introduction}

O objetivo central da área de complexidade computacional é saber a dificuldade intrínseca de problemas computacionais, diferentemente de design de algoritmos que busca encontrar soluções eficientes para um problema. Quando falamos de complexidade de problemas computacionais estamos querendo dizer o recurso necessário para resolver tais problemas. 

Segue então que ao analisar a complexidade de problemas computacionais nós devemos levar em conta o modelo computacional e o recurso em questão. Às vezes estamos interessados em avaliar o tempo necessário para computar um certo problema em uma máquina de acesso aleatório ou talvez o número de bits que dois processadores enviam um ao outro. Alguns resultados antigos mostram que dar mais tempo ou espaço para máquinas de Turing aumenta o número de problemas que elas podem resolver, porém estes resultados não apresentaram limites inferiores para problemas naturais~\cite{arora2009computational}. Meyer e Stockmeyer provaram que certos problemas são completos para a classe de problemas que necessitam de tempo exponencial em máquinas de Turing~\cite{beyondNP}, o que também significa que estes problemas não podem ser decididos em tempo polinomial.

Em complexidade de circuitos procura-se saber o tamanho ou profundidade dos circuitos necessários para decidir uma linguagem. Circuitos booleanos são matematicamente mais simples do que máquinas de Turing e alguns resultados em complexidade de circuitos resolveriam também problemas em aberto em outros modelos de computação. Como exemplo, se conseguirmos provar que problemas que são fáceis de se verificar uma solução não têm circuitos de tamanho polinomial ($\NP \nsubseteq \Ppoly$) então $\P \neq \NP$. Porém, provar que $\NP \nsubseteq \Ppoly$ é extremamente dificil e por isso o foco de pesquisa hoje em dia é provar problemas mais fracos, na maioria das vezes restrigindo a classe de circuitos (como por exemplo, circuitos de profundidade constante). Nos anos 80 houve avanços neste sentido quando pesquisadores conseguiram mostrar que certos problemas não podem ser resolvidos por classes mais restritas de circuitos. No entanto, em 1994, Razborov e Rudich mostraram que, sob algumas hipóteses de complexidade computacional, as técnicas usadas até então para provar limites inferiores, as quais eles chamaram de provas naturais, não seriam suficiente para provar que $\P \neq \NP$~\cite{natural}. E por isso, para que qualquer estratégia de prova possa ser levada a frente é necessário de alguma forma passar pelas limitações das provas naturais.

Nos últimos anos, novos limites inferiores em complexidade de circuitos foram obtidos usando uma estratégia de prova que liga algoritmos ``rápidos''  a limite inferiores ~\cite{RW1, RW2}.  Para uma determinada classe de circuitos $C$, se você conseguir mostrar que o problema $C$--$\SAT$ (o problema de avaliar se um circuito em $C$ não computa a função $f(x) = 0$, para todo $x$) tem um algoritmo mais rápido do que o algoritmo mais óbvio (tentar todas as $2^n$ entradas possíveis), então você consegue mostrar que a classe de problemas cuja uma solução pode ser verificada em tempo exponencial ($\NEXP$) não tem circuitos em $C$. Estudar a conexão entre algoritmos e limites inferiores em circuitos booleanos é um assunto interessante para alguém que deseja realizar pesquisas em complexidade computacional.

Com o objetivo de fazer este texto autocontido no capítulo \ref{chapter_fundaments} nós fazemos um resumo dos fundamentos básicos necessários nos capítulos subsequentes, o que inclui algumas definições matemáticas. Em \ref{section_turing_machines} iremos ver máquinas de Turing e algumas de suas generalizações e mostraremos que enquanto máquinas de Turing são poderosas o suficiente para capturar tudo que consideramos computável, ainda existem problemas que nos interessariam que não são computáveis por máquinas de Turing. Depois em \ref{section_boolean_circuits} nós damos uma definição formal de circuitos Booleanos e definimos alguns conceitos importantes relacionados a este modelo de computação. Em \ref{section_computational_complexity} nós começamos a falar de complexidade computacional e apresentamos algumas classes de complexidade que iremos ver durante este trabalho como $\P$, $\NP$ e $\PSPACE$. Também apresentamos alguns resultados clássicos como alguns teoremas de hierarquia e o resultado de Baker, Gill e Solovay que prova a existência de um oráculo relativas a qual $\P$ e $\NP$ são diferentes. A seção \ref{section_circuit_complexity} é sobre complexidade de circuitos e assim como vimos classes de complexidades definidas a partir de máquinas de turing nós iremos ver algumas classes de complexidades de circuitos, como elas se relacionam com outras classes de complexidades que vimos anteriormente e provaremos alguns resultados importantes que podem servir de motivação para outros resultados que irão aparecer mais para frente.

\subsubsection{Computação relativizada e complexidade de circuitos}

No capítulo \ref{chapter_oracles_and_circuit_complexity} nós iremos ver como separações de classes de complexidade com oráculos seguem de alguns limitantes inferiores em complexidade de circuitos usando ideias que são discutidas em~\cite{ko1989constructing} e~\cite{DBLP:journals/sigact/RossmanST15}. Em \ref{section_alternative_proof_bgs} nós provamos de novo o teorema de Baker, Gill e Solovay desta vez usando o limitante inferior para a complexidade de consulta da função $\Tribes_{N}$ definida como

\begin{equation*}
    \Tribes_{N}(x_{1, 1}, x_{1, 2}, \dots, x_{2^{n}, n - 1}, x_{2^{n}, n}) = \bigvee_{i = 1}^{2^{n}} \bigwedge_{j = 1}^{n} x_{i, j},
\end{equation*}

em que $N = n2^{n}$. A complexidade de consulta de uma função $\binalph^{n} \to \binalph$ é definida como a profundidade mínima entre todas as árvores de decisão que computam a função. Nós facilmente podemos provar que a função $\Tribes_{N}$ tem complexidade de consulta máxima $N$. Nos anos 80 pesquisadores estavam interessados em saber a relação entre outras classes de complexidades relativas a algum oráculo. Em especial eles queriam saber se existe algum oráculo que separa as classes $\PSPACE$ e $\PH$ e também se existe algum oráculo relativo a qual a hierarquia polinomial é infinita. Mostrando que a hierarquia polinomial pode ser expressa por circuitos de profundidade constante obtém-se que estes resultados seguem a partir de limitantes inferiores para o tamanho de circuitos de profundidade constante que computam determinadas funções, sendo estas as funções $\Parity_{n}$ e as funções de Sipser que iremos denotar por $f^{m, d}$. A definição destas duas funções aparece em \ref{parity} e \ref{Sipser_f} respectivamente. Nós iremos ver uma prova das seguintes implicações.

\begin{itemize}

    \item As funções $\Parity_{n}$ exigem circuitos de profundidade constante de tamanho superpolinomial $\Rightarrow$ existe um oráculo $A$ tal que $\PH^{A} \neq \PSPACE^{A}$.
    
    \item As funções de Sipser $f^{m, d}$ exigem circuitos de profundidade constante de tamanho superpolinomial $\Rightarrow$ existe um oráculo $A$ tal que a hierarquia polinomial é infinita relativa a $A$.

\end{itemize}

Nós deixamos as provas dos dois limitantes inferiores acima para o capítulo \ref{chapter_random_restrictions_and_projections}. Ao provar cada um destes limitantes inferiores nós também obteremos que as funções $\Parity_{n}$ e as funções de Sipser $f^{m, d}$ não podem nem mesmo ser aproximadas por circuitos de profundidade constante e tamanho polinomial. Pela lei zero-um de Kolmogorov nós podemos ainda provar as seguintes implicações.

\begin{itemize}

    \item As funções $\Parity_{n}$ não podem ser aproximadas por circuitos de profundidadade constante e tamanho polinomial $\Rightarrow$ $\PH^{A} \neq \PSPACE^{A}$ para um oráculo aleatório com probabilidade 1.
    
    \item As funções de Sipser $f^{m, d}$ não podem ser aproximadas por circuitos de profundidade constante e tamanho polinomial $\Rightarrow$ a hierarquia polinomial é infinita relativa a um oráculo aleatório com probabilidade 1.

\end{itemize}

Dizer que, por exemplo, $\PSPACE$ e $\PH$ são diferentes relativas a um oráculo aleatório com probabilidade 1 não significa que elas são diferentes relativas a todo os oráculos, mas sim significa que o conjunto de todos os oráculos $A$ que satisfazem $\PH^{A} = \PSPACE^{A}$ tem medida zero. De fato, veremos que para todos os resultados do capítulo \ref{chapter_oracles_and_circuit_complexity} existe um oráculo relativa a qual as classes de complexidade em questão colapsam. A importância de um resultado destes é que se quisermos provar relações entre estas classes nós necessariamente teremos que usar métodos de provas que não relativizam, o que basicamente significa que teremos que usar um argumento que usa algo a mais do que a capacidade de uma das classes poder simular a outra classe. No fim deste capítulo nós iremos ver um argumento que duas classes de complexidades serem diferentes relativas a um oráculo aleatório nem sequer serve como evidência que estas duas classes são diferentes no mundo não relativizado~\cite{fortnow1994role}.

\subsubsection{Restrições e projeções aleatórias}

O capítulo \ref{chapter_random_restrictions_and_projections} é inteiramente voltado para provar os limitantes inferiores enunciados no capítulo \ref{chapter_oracles_and_circuit_complexity} (ver \ref{teo: parity_lb}, \ref{teo: parity_lb_app}, \ref{Sipser_f_lb} e \ref{Sipser_f_lb_app}). Na seção \ref{section_hastad_proofs} nós iremos ver uma prova que as funções $\Parity_{n}$ exigem circuitos de profundidade constante que tenham um número exponencial de portas lógicas. O método utilizado é o de restrições aleatórias introduzidos em~\cite{subbotovskaya1961realizations} que funciona da seguinte forma. Suponha que tenhas uma função Booleana $f: \binalph^{n} \to \binalph$ com variáveis de entrada $x_{1}, x_{2}, \dots, x_{n}$. Uma restrição aleatória $\rho$ irá atribuir valores em $\{0, 1, *\}$ de forma aleatória e independente para cada uma das $n$ variáveis de $f$, em que $*$ denota que a variável permanece livre. Chame de $\rho(x_{i})$ o valor atribuido à $i$-ésima variável, então a função resultante $f_{\lvert \rho}$ satisfaz

\begin{equation*} 
    f_{\lvert \rho}(x_{1}, x_{2}, \dots, x_{n}) = 1 \iff f(\rho(x_{1}), \rho(x_{2}), \dots, \rho(x_{n})) = 1.
\end{equation*}

Agora, o que queremos provar é que todos circuitos de profundidade constante e tamanho subexponencial não podem computar $\Parity_{n}$, para $n$ suficientemente grande. Uma forma de fazer isto é provar os seguintes pontos separadamente.

\begin{enumerate}

    \item Toda função computada por um circuito de profundidade constante e tamanho subexponencial colapsa para uma função extremamente simples quando atingida por uma restrição aleatória
    
    \item As funções $\Parity_{n}$ permanecem complexas mesmo após serem atingidas por uma restrição aleatória.

\end{enumerate}


H{\aa}stad em sua tese de doutorado~\cite{haastad1987computational} provou o lema da troca de H{\aa}stad \ref{hastad_lemma}, que é essencial para provar o primeiro ponto acima. Nós iremos ver duas provas do lema da troca de H{\aa}stad, sendo a primeira prova a original de H{\aa}stad que usa indução. A segunda prova, que é atribuida a Razborov, aparece em~\cite{beame1994switching} e usa um argumento de contagem. O lema diz que circuitos de profundidade 2 simpĺificam quando atingidos por uma restrição aleatória. Então podemos para cada camada de um circuito de profundidade constante aplicar uma restrição aleatória que com probabilidade muito alta irá simplificar esta camada ao ponto de reduzir a profundidade do circuito em 1. Ao fim teremos transformado um circuito de profundidade $d \geq 2$ em algo como uma árvore de decisão de profundidade constante. Por outro lado, o item (2) acima é facilmente obtido observando que ao restringir as variáveis de entrada da função $\Parity_{n}$ nós ainda temos uma função paridade ou a sua negação sobre um número menor de variáveis mas que ainda não pode ser expressa por uma árvore de decisão de profundidade constante. Para provar o teorema \ref{teo: parity_lb_app} nós iremos usar o fato que restrições aleatórias preservam a distribuição uniforme.

H{\aa}stad conseguiu ainda na sua tese doutorado provar o teorema \ref{Sipser_f_lb} usando restrições em bloco que atribui valores às variáveis de entrada de forma que variáveis num mesmo bloco não têm valores atribuidos de forma independente. Porém o método dele não é suficiente para provar \ref{Sipser_f_lb_app} pois restrições em bloco não preservam a distribuição uniforme. Em~\cite{rossman2015average}, Rossman, Servedio e Tan conseguiram superar esta barreira para por fim provar \ref{Sipser_f_lb_app}. Na seção \ref{section_RST_proof} iremos mostrar a prova de Rossman, Servedio e Tan dos teoremas \ref{Sipser_f_lb} e \ref{Sipser_f_lb_app} que usa projeções aleatórias que são uma generalização de restrições aleatórias. Agora ao invés de setar uma variável para uma constante ou manter elas livres, nós iremos particionar elas em blocos de forma que cada variável ou é feita constante ou é projetada para uma nova variável que é comum à todas as outras variáveis no mesmo bloco.

\section{Objetivo}

Este trabalho visa apresentar um estudo teórico acerca da área de complexidade de circuitos e suas aplicações ao estudo da complexidade computacional. Primeiramente será feito um estudo sobre complexidade computacional. Depois serão pesquisados tópicos em complexidades de circuitos com foco principal nos tópicos mais recentes e ferramentas/técnicas comumente usadas em provas em complexidade de circuitos.
